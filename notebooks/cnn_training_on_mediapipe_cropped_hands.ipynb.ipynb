{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84345e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ba3c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\dl_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Loss: 3.9015 | Train Acc: 0.0760 | Val Acc: 0.1762\n",
      "Epoch 2/50 | Loss: 2.6930 | Train Acc: 0.1793 | Val Acc: 0.3502\n",
      "Epoch 3/50 | Loss: 2.1266 | Train Acc: 0.3124 | Val Acc: 0.6564\n",
      "Epoch 4/50 | Loss: 1.7150 | Train Acc: 0.4146 | Val Acc: 0.7423\n",
      "Epoch 5/50 | Loss: 1.4637 | Train Acc: 0.4967 | Val Acc: 0.7489\n",
      "Epoch 6/50 | Loss: 1.2963 | Train Acc: 0.5399 | Val Acc: 0.8084\n",
      "Epoch 7/50 | Loss: 1.1863 | Train Acc: 0.5848 | Val Acc: 0.8623\n",
      "Epoch 8/50 | Loss: 1.0718 | Train Acc: 0.6193 | Val Acc: 0.8800\n",
      "Epoch 9/50 | Loss: 0.9709 | Train Acc: 0.6499 | Val Acc: 0.8590\n",
      "Epoch 10/50 | Loss: 0.9280 | Train Acc: 0.6672 | Val Acc: 0.8877\n",
      "Epoch 11/50 | Loss: 0.9070 | Train Acc: 0.6658 | Val Acc: 0.9020\n",
      "Epoch 12/50 | Loss: 0.8305 | Train Acc: 0.6871 | Val Acc: 0.9020\n",
      "Epoch 13/50 | Loss: 0.7822 | Train Acc: 0.7050 | Val Acc: 0.9196\n",
      "Epoch 14/50 | Loss: 0.7689 | Train Acc: 0.7240 | Val Acc: 0.9009\n",
      "Epoch 15/50 | Loss: 0.7358 | Train Acc: 0.7157 | Val Acc: 0.9306\n",
      "Epoch 16/50 | Loss: 0.6984 | Train Acc: 0.7386 | Val Acc: 0.9416\n",
      "Epoch 17/50 | Loss: 0.6657 | Train Acc: 0.7526 | Val Acc: 0.9240\n",
      "Epoch 18/50 | Loss: 0.6527 | Train Acc: 0.7551 | Val Acc: 0.9383\n",
      "Epoch 19/50 | Loss: 0.6335 | Train Acc: 0.7573 | Val Acc: 0.9493\n",
      "Epoch 20/50 | Loss: 0.6016 | Train Acc: 0.7727 | Val Acc: 0.9317\n",
      "Epoch 21/50 | Loss: 0.5585 | Train Acc: 0.7862 | Val Acc: 0.9416\n",
      "Epoch 22/50 | Loss: 0.5617 | Train Acc: 0.7879 | Val Acc: 0.9438\n",
      "Epoch 23/50 | Loss: 0.5577 | Train Acc: 0.7915 | Val Acc: 0.9416\n",
      "Epoch 24/50 | Loss: 0.5206 | Train Acc: 0.8030 | Val Acc: 0.9449\n",
      "Epoch 25/50 | Loss: 0.5138 | Train Acc: 0.8011 | Val Acc: 0.9482\n",
      "Epoch 26/50 | Loss: 0.4519 | Train Acc: 0.8322 | Val Acc: 0.9526\n",
      "Epoch 27/50 | Loss: 0.4259 | Train Acc: 0.8386 | Val Acc: 0.9581\n",
      "Epoch 28/50 | Loss: 0.4266 | Train Acc: 0.8339 | Val Acc: 0.9570\n",
      "Epoch 29/50 | Loss: 0.4019 | Train Acc: 0.8397 | Val Acc: 0.9626\n",
      "Epoch 30/50 | Loss: 0.3805 | Train Acc: 0.8562 | Val Acc: 0.9648\n",
      "Epoch 31/50 | Loss: 0.4177 | Train Acc: 0.8408 | Val Acc: 0.9559\n",
      "Epoch 32/50 | Loss: 0.3645 | Train Acc: 0.8592 | Val Acc: 0.9548\n",
      "Epoch 33/50 | Loss: 0.3985 | Train Acc: 0.8471 | Val Acc: 0.9626\n",
      "Epoch 34/50 | Loss: 0.3658 | Train Acc: 0.8606 | Val Acc: 0.9626\n",
      "Epoch 35/50 | Loss: 0.3724 | Train Acc: 0.8537 | Val Acc: 0.9515\n",
      "Epoch 36/50 | Loss: 0.3739 | Train Acc: 0.8545 | Val Acc: 0.9548\n",
      "Epoch 37/50 | Loss: 0.3294 | Train Acc: 0.8725 | Val Acc: 0.9626\n",
      "Epoch 38/50 | Loss: 0.3283 | Train Acc: 0.8719 | Val Acc: 0.9637\n",
      "Epoch 39/50 | Loss: 0.3243 | Train Acc: 0.8738 | Val Acc: 0.9648\n",
      "Epoch 40/50 | Loss: 0.3260 | Train Acc: 0.8741 | Val Acc: 0.9637\n",
      "Epoch 41/50 | Loss: 0.3308 | Train Acc: 0.8722 | Val Acc: 0.9648\n",
      "Epoch 42/50 | Loss: 0.3225 | Train Acc: 0.8752 | Val Acc: 0.9604\n",
      "Epoch 43/50 | Loss: 0.3224 | Train Acc: 0.8802 | Val Acc: 0.9648\n",
      "Epoch 44/50 | Loss: 0.2958 | Train Acc: 0.8857 | Val Acc: 0.9626\n",
      "Epoch 45/50 | Loss: 0.3085 | Train Acc: 0.8802 | Val Acc: 0.9615\n",
      "Epoch 46/50 | Loss: 0.3102 | Train Acc: 0.8774 | Val Acc: 0.9637\n",
      "Epoch 47/50 | Loss: 0.3320 | Train Acc: 0.8683 | Val Acc: 0.9604\n",
      "Epoch 48/50 | Loss: 0.3068 | Train Acc: 0.8744 | Val Acc: 0.9659\n",
      "Epoch 49/50 | Loss: 0.3158 | Train Acc: 0.8791 | Val Acc: 0.9615\n",
      "Epoch 50/50 | Loss: 0.2958 | Train Acc: 0.8824 | Val Acc: 0.9670\n",
      "------------------------------\n",
      "Best Validation Accuracy achieved: 0.9670\n",
      "Model saved at: best_hand_cnn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_23208\\1854125440.py:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_hand_cnn_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9934\n"
     ]
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset + DataLoaders\n",
    "# ---------------------------\n",
    "data_dir = \"processed_cropped\"\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=train_transforms)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "val_dataset.dataset.transform = val_transforms\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = datasets.ImageFolder(root=\"processed_cropped_test\", transform=test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# ---------------------------\n",
    "# CNN\n",
    "# ---------------------------\n",
    "class EnhancedCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*8*8, 512), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "num_classes = len(os.listdir(data_dir))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EnhancedCNN(num_classes=num_classes).to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# Loss + Optimizer + Scheduler\n",
    "# ---------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Training Loop\n",
    "# ---------------------------\n",
    "num_epochs = 50\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss, running_corrects = 0.0, 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(torch.argmax(outputs, 1) == labels)\n",
    "\n",
    "    epoch_loss = running_loss / train_size\n",
    "    epoch_acc = running_corrects.double() / train_size\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_corrects = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_corrects += torch.sum(torch.argmax(outputs, 1) == labels)\n",
    "\n",
    "    val_acc = val_corrects.double() / val_size\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_hand_cnn_model.pth\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(f\"Best Validation Accuracy achieved: {best_val_acc:.4f}\")\n",
    "print(\"Model saved at: best_hand_cnn_model.pth\")\n",
    "\n",
    "# ---------------------------\n",
    "# Test Accuracy\n",
    "# ---------------------------\n",
    "model.load_state_dict(torch.load(\"best_hand_cnn_model.pth\"))\n",
    "model.eval()\n",
    "test_corrects = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_corrects += torch.sum(torch.argmax(outputs, 1) == labels)\n",
    "\n",
    "test_acc = test_corrects.double() / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb29c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\dl_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | Loss: 3.7520 | Train Acc: 0.0820 | Val Acc: 0.1731\n",
      "Epoch 02/50 | Loss: 2.7251 | Train Acc: 0.1805 | Val Acc: 0.3626\n",
      "Epoch 03/50 | Loss: 2.2638 | Train Acc: 0.2729 | Val Acc: 0.5659\n",
      "Epoch 04/50 | Loss: 1.8666 | Train Acc: 0.3848 | Val Acc: 0.6648\n",
      "Epoch 05/50 | Loss: 1.6070 | Train Acc: 0.4561 | Val Acc: 0.7500\n",
      "Epoch 06/50 | Loss: 1.4245 | Train Acc: 0.4995 | Val Acc: 0.8077\n",
      "Epoch 07/50 | Loss: 1.3195 | Train Acc: 0.5197 | Val Acc: 0.8132\n",
      "Epoch 08/50 | Loss: 1.2414 | Train Acc: 0.5607 | Val Acc: 0.8434\n",
      "Epoch 09/50 | Loss: 1.1611 | Train Acc: 0.5788 | Val Acc: 0.8709\n",
      "Epoch 10/50 | Loss: 1.1682 | Train Acc: 0.5794 | Val Acc: 0.8379\n",
      "Epoch 11/50 | Loss: 1.0688 | Train Acc: 0.6139 | Val Acc: 0.8984\n",
      "Epoch 12/50 | Loss: 0.9853 | Train Acc: 0.6338 | Val Acc: 0.9011\n",
      "Epoch 13/50 | Loss: 0.9259 | Train Acc: 0.6571 | Val Acc: 0.9203\n",
      "Epoch 14/50 | Loss: 0.9391 | Train Acc: 0.6519 | Val Acc: 0.9341\n",
      "Epoch 15/50 | Loss: 0.8645 | Train Acc: 0.6718 | Val Acc: 0.9176\n",
      "Epoch 16/50 | Loss: 0.8752 | Train Acc: 0.6678 | Val Acc: 0.9066\n",
      "Epoch 17/50 | Loss: 0.8735 | Train Acc: 0.6687 | Val Acc: 0.9286\n",
      "Epoch 18/50 | Loss: 0.7987 | Train Acc: 0.6999 | Val Acc: 0.9368\n",
      "Epoch 19/50 | Loss: 0.7645 | Train Acc: 0.7085 | Val Acc: 0.9478\n",
      "Epoch 20/50 | Loss: 0.7155 | Train Acc: 0.7366 | Val Acc: 0.9313\n",
      "Epoch 21/50 | Loss: 0.7428 | Train Acc: 0.7149 | Val Acc: 0.9533\n",
      "Epoch 22/50 | Loss: 0.7319 | Train Acc: 0.7225 | Val Acc: 0.9451\n",
      "Epoch 23/50 | Loss: 0.6905 | Train Acc: 0.7363 | Val Acc: 0.9423\n",
      "Epoch 24/50 | Loss: 0.6908 | Train Acc: 0.7336 | Val Acc: 0.9643\n",
      "Epoch 25/50 | Loss: 0.6988 | Train Acc: 0.7287 | Val Acc: 0.9368\n",
      "Epoch 26/50 | Loss: 0.6659 | Train Acc: 0.7492 | Val Acc: 0.9615\n",
      "Epoch 27/50 | Loss: 0.6088 | Train Acc: 0.7703 | Val Acc: 0.9588\n",
      "Epoch 28/50 | Loss: 0.6706 | Train Acc: 0.7369 | Val Acc: 0.9533\n",
      "Epoch 29/50 | Loss: 0.6420 | Train Acc: 0.7541 | Val Acc: 0.9505\n",
      "Epoch 30/50 | Loss: 0.6268 | Train Acc: 0.7599 | Val Acc: 0.9643\n",
      "Epoch 31/50 | Loss: 0.5446 | Train Acc: 0.7901 | Val Acc: 0.9643\n",
      "Epoch 32/50 | Loss: 0.5398 | Train Acc: 0.7889 | Val Acc: 0.9698\n",
      "Epoch 33/50 | Loss: 0.5222 | Train Acc: 0.8002 | Val Acc: 0.9698\n",
      "Epoch 34/50 | Loss: 0.4928 | Train Acc: 0.8009 | Val Acc: 0.9698\n",
      "Epoch 35/50 | Loss: 0.5024 | Train Acc: 0.8042 | Val Acc: 0.9725\n",
      "Epoch 36/50 | Loss: 0.4775 | Train Acc: 0.8134 | Val Acc: 0.9643\n",
      "Epoch 37/50 | Loss: 0.4819 | Train Acc: 0.8054 | Val Acc: 0.9670\n",
      "Epoch 38/50 | Loss: 0.4934 | Train Acc: 0.8021 | Val Acc: 0.9643\n",
      "Epoch 39/50 | Loss: 0.4799 | Train Acc: 0.8058 | Val Acc: 0.9643\n",
      "Epoch 40/50 | Loss: 0.4937 | Train Acc: 0.8061 | Val Acc: 0.9670\n",
      "Epoch 41/50 | Loss: 0.4732 | Train Acc: 0.8100 | Val Acc: 0.9698\n",
      "Epoch 42/50 | Loss: 0.4708 | Train Acc: 0.8097 | Val Acc: 0.9698\n",
      "Epoch 43/50 | Loss: 0.4456 | Train Acc: 0.8220 | Val Acc: 0.9698\n",
      "Epoch 44/50 | Loss: 0.4612 | Train Acc: 0.8189 | Val Acc: 0.9670\n",
      "Epoch 45/50 | Loss: 0.4354 | Train Acc: 0.8256 | Val Acc: 0.9753\n",
      "Epoch 46/50 | Loss: 0.4596 | Train Acc: 0.8128 | Val Acc: 0.9670\n",
      "Epoch 47/50 | Loss: 0.4283 | Train Acc: 0.8333 | Val Acc: 0.9698\n",
      "Epoch 48/50 | Loss: 0.4304 | Train Acc: 0.8272 | Val Acc: 0.9725\n",
      "Epoch 49/50 | Loss: 0.4266 | Train Acc: 0.8262 | Val Acc: 0.9725\n",
      "Epoch 50/50 | Loss: 0.4351 | Train Acc: 0.8305 | Val Acc: 0.9670\n",
      "----------------------------------------\n",
      "Best Validation Accuracy: 0.9753\n",
      "Model saved at: ..\\models\\best_hand_cnn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_7560\\1018904116.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test Accuracy: 0.9582\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/processed/processed_cropped_mp\"\n",
    "\n",
    "models_dir = os.path.join(\"..\", \"models\")\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Transforms\n",
    "# ---------------------------\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset\n",
    "# ---------------------------\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=train_transforms)\n",
    "targets = full_dataset.targets\n",
    "num_classes = len(full_dataset.classes)\n",
    "\n",
    "all_indices = list(range(len(full_dataset)))\n",
    "\n",
    "# Train / Test split (80 / 20)\n",
    "train_indices, test_indices = train_test_split(\n",
    "    all_indices,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=targets\n",
    ")\n",
    "\n",
    "# Train / Val split (90 / 10 of train)\n",
    "train_indices, val_indices = train_test_split(\n",
    "    train_indices,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=[targets[i] for i in train_indices]\n",
    ")\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset   = Subset(full_dataset, val_indices)\n",
    "test_dataset  = Subset(full_dataset, test_indices)\n",
    "\n",
    "# Change transforms for val & test\n",
    "val_dataset.dataset.transform  = val_transforms\n",
    "test_dataset.dataset.transform = test_transforms\n",
    "\n",
    "# ---------------------------\n",
    "# DataLoaders\n",
    "# ---------------------------\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,  num_workers=2)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# ---------------------------\n",
    "# CNN Model\n",
    "# ---------------------------\n",
    "class EnhancedCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ---------------------------\n",
    "# Device\n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EnhancedCNN(num_classes).to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# Loss / Optimizer / Scheduler\n",
    "# ---------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Training Loop\n",
    "# ---------------------------\n",
    "num_epochs = 50\n",
    "best_val_acc = 0.0\n",
    "model_path = os.path.join(models_dir, \"best_hand_cnn_model.pth\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(torch.argmax(outputs, 1) == labels)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc  = running_corrects.double() / len(train_dataset)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_corrects += torch.sum(torch.argmax(outputs, 1) == labels)\n",
    "\n",
    "    val_acc = val_corrects.double() / len(val_dataset)\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    # ---- Save best model ----\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02}/{num_epochs} | \"\n",
    "        f\"Loss: {epoch_loss:.4f} | \"\n",
    "        f\"Train Acc: {epoch_acc:.4f} | \"\n",
    "        f\"Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Model saved at: {model_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Test Accuracy\n",
    "# ---------------------------\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "test_corrects = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_corrects += torch.sum(torch.argmax(outputs, 1) == labels)\n",
    "\n",
    "test_acc = test_corrects.double() / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
